{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set: Information Retrieval\n",
    "\n",
    "## Part 2: Structuring + Manipulating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set, you'll be continuing to work with the data you imported during lecture. You'll be structuring and manipulating that data to turn it into a tabular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from operator import itemgetter\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface: Run Code from Lecture\n",
    "\n",
    "Run the code below to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge lines so that each number starts with a number\n",
    "def mergeLines(l):\n",
    "    '''\n",
    "    This function takes in a list of lines `l` and merge broken paragraph lines \n",
    "    (merge all lines if they don't start with a number)\n",
    "    '''\n",
    "    i = 0\n",
    "    while i < len(l):\n",
    "        if not l[i][0].isdigit():\n",
    "            l[i-1:i+1] = [' '.join(l[i-1:i+1])]\n",
    "        else:\n",
    "            i = i+1\n",
    "    return(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing afghanistan2014.txt...\n",
      "processing albania2014.txt...\n",
      "processing angola2014.txt...\n",
      "processing bhutan2014.txt...\n",
      "processing bolivia2014.txt...\n",
      "processing bosniaandherzegovina2014.txt...\n",
      "processing bruneidarussalam2014.txt...\n",
      "processing cambodia2014.txt...\n",
      "processing chile2014.txt...\n",
      "processing comoros2014.txt...\n",
      "processing costarica2014.txt...\n",
      "processing cotedivoire2014.txt...\n",
      "processing cyprus2014.txt...\n",
      "processing democraticpeoplesrepublicofkorea2014.txt...\n",
      "processing democraticrepublicofthecongo2014.txt...\n",
      "processing dominica2014.txt...\n",
      "processing dominicanrepublic2014.txt...\n",
      "processing egypt2014.txt...\n",
      "processing elsalvador2014.txt...\n",
      "processing eritrea2014.txt...\n",
      "processing ethiopia2014.txt...\n",
      "processing fiji2014.txt...\n",
      "processing gambia2014.txt...\n",
      "processing guinea2014.txt...\n",
      "processing iran2014.txt...\n",
      "processing iraq2014.txt...\n",
      "processing italy2014.txt...\n",
      "processing kazakhstan2014.txt...\n",
      "processing Macedonia2014.txt...\n",
      "processing madagascar2014.txt...\n",
      "processing newzealand2014.txt...\n",
      "processing nicaragua2014.txt...\n",
      "processing norway2014.txt...\n",
      "processing portugal2014.txt...\n",
      "processing qatar2014.txt...\n",
      "processing sanmarino2014.txt...\n",
      "processing slovakia2014.txt...\n",
      "processing slovenia2014.txt...\n",
      "processing uruguay2014.txt...\n",
      "processing vanuatu2014.txt...\n",
      "processing vietnam2014.txt...\n",
      "processing yemen2014.txt...\n"
     ]
    }
   ],
   "source": [
    "l=[]\n",
    "dir = 'data/txts'\n",
    "for file_name in os.listdir(dir):\n",
    "    broken = []\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        print 'processing ' + file_name + '...'\n",
    "        try:\n",
    "            dic = {}\n",
    "            dic['country'] = file_name[:-8]\n",
    "            dic['year'] = file_name[-8:-4]\n",
    "            f = open(dir + '/' + file_name,'rU')\n",
    "            text = f.read() # read in text\n",
    "            f.close\n",
    "            text = text.split('\\n') # make a list\n",
    "            text = filter(None, text) # get rid of empty string items       \n",
    "             \n",
    "            # take only the conclusions and/or recommendations section\n",
    "            ConclusionsStart = text.index([line for line in text if \"conclusions and/or recommendations\" in line.lower()][1]) #startin from bottom\n",
    "            ConclusionsEnd = text.index([line for line in text if \"conclusions and/or recommendations\" in line.lower()][2]) # the last one is the disclaimer\n",
    "            text = text[ConclusionsStart+1:ConclusionsEnd+1] \n",
    "            \n",
    "            # get rid of the weird lines\n",
    "            text = [line for line in text if '**' not in line]\n",
    "            text = [line for line in text if 'recommendations have not been edited.' not in line]\n",
    "            text = [line for line in text if 'recommendations will not be edited.' not in line]\n",
    "            text = [line.replace('\\xd2','') for line in text]\n",
    "            text = [line.replace('\\t','') for line in text]\n",
    "            text = [line.lstrip(\" \") for line in text]\n",
    "            \n",
    "            # merge lines so that each line is its own paragraph, starting with a paragraph number\n",
    "            text = mergeLines(text)\n",
    "            \n",
    "            # get rid of that disclaimer paragraph\n",
    "            text = [line for line in text if 'endorsed by the working group' not in line.lower()]\n",
    "            \n",
    "            dic['text'] = text \n",
    "            \n",
    "            # append to list\n",
    "            l.append(dic)\n",
    "              \n",
    "        except Exception,e:\n",
    "            broken.append(file_name +str(e)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.0 We'll first be working with a single UPR report. Make an object `upr` that contains the fifth item in the list `l.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign `upr` as the fourth item in the list\n",
    "upr = l[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These texts have sections for conclusions/recommendations the country either accepts, rejects, considers, etc. So the first task is to split the document into sections pertaining to those decisions.\n",
    "\n",
    "The problem is, we don't know how many sections a document has a priori. Luckily, Rochelle wrote you a function that will tell how you many sections there are in a document. \n",
    "\n",
    "1.1 Using this function, how many sections are there in this `upr`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to find main paragraphs numbers in each upr\n",
    "def mainParagraphs(upr):\n",
    "    '''\n",
    "    This function takes in a upr and returns the main paragraph numbers in the 'recommendations' section.\n",
    "    There are usually 2-4 main paragraphs. Sometimes I refer to these main paragraph sections as \"chunks\".\n",
    "    '''\n",
    "    firstParagraph = upr['text'][0].partition(\" \")[0]\n",
    "    if '.' in firstParagraph:\n",
    "            firstParagraph = firstParagraph.replace(\".\",\"\")\n",
    "    \n",
    "    mainParagraphs = []\n",
    "    for line in upr['text']:\n",
    "        paragraph = line.partition(\" \")[0]\n",
    "        if paragraph[-1] == '.':\n",
    "            paragraph = paragraph[:-1]\n",
    "                \n",
    "        mainParagraphs.append(float(paragraph))\n",
    "        \n",
    "    # make a list of the main paragraph numbers\n",
    "    mainParagraphs = set([int(n) for n in mainParagraphs if int(n)>= int(firstParagraph)])\n",
    "    return mainParagraphs \n",
    "\n",
    "# Uncomment to test\n",
    "# mainParagraphs(upr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 In your own words, explain how this function works. Feel free to make new cells and explore each line of the function, and/or litter the function with print statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Now let's split up the text into sections. Create a new empty list called `sections`. For each section in the `upr`, create a dictionary containing keys for `paragraph-number`, containing the main paragraph number; and `full-text`, containing the all the sub-paragraphs in that section. Your final list should look something like this:\n",
    "\n",
    "```\n",
    "[\n",
    "    { \"paragraph-number\": 101,\n",
    "      \"full-text\": [ 101. Some text, 101.1 More text, 101.2 Another line, 101.3 Even more] } ,\n",
    "    \n",
    "    { \"paragraph-number\": 102,\n",
    "      \"full-text\": [ 102. A new line, 102.1 Some text, 102.2 Even more text, 102.3 You get the picture ] }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sections = []\n",
    "for n in mainParagraphs(upr):\n",
    "    dic = {}\n",
    "    dic['paragraph'] = n\n",
    "    dic['text'] = [s for s in upr['text'] if s.startswith(str(n))]\n",
    "    sections.append(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Some sections aren't really sections -- they're just single paragraphs with no subitems. We should delete those, because they don't contain any actual recommendations. Loop through the sections, and if a section contains only one line (meaning it has no subitems), delete that section. How many sections are we left with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete all the chunks with only 1 paragraph\n",
    "for dic in sections:\n",
    "    if len(dic['text']) == 1:\n",
    "        sections.remove(dic)\n",
    "len(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing into main text and subitems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Take a look at one of the sections in the `sections` list. For each section, the first paragraph is the header paragraph, explaining how these recommendations are linked (e.g. they've all been accepted or rejected). Below that are the items belonging to that section, containing specific recommendations. We want to parse the sections to capture this structure, dividing the lines into 'header' and 'items'.\n",
    "\n",
    "Looping through each dictionary in `sections` list, add the keys `header` and `items` to the dictionary, with their respective lines. The `header` key should contain only one line, while the `items` key should contain a list of lines. In your final result, the `sections` list should look like this:\n",
    "\n",
    "\n",
    "```\n",
    "[\n",
    "    { \"paragraph-number\": 101,\n",
    "      \"full-text\": [ 101. Some text, 101.1 More text, 101.2 Another line, 101.3 Even more]\n",
    "      \"header\": 101. Some text\n",
    "      \"items\": [101.1 More text, 101.2 Another line, 101.3 Even more] } ,\n",
    "     \n",
    "    { \"paragraph-number\": 102,\n",
    "      \"full-text\": [ 102. A new section, 102.1 Some text, 102.2 Even more text, 102.3 You get the picture ] \n",
    "      \"header\": 102. A new section\n",
    "      \"items\": [102.1 Some text, 102.2 Even more text, 102.3 You get the picture] }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'113.  The recommendations formulated during the interactive dialogue/listed below have been examined by the Plurinational State of Bolivia and enjoy the support of the State party: '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dic in sections:\n",
    "    dic['header'] = dic['text'][0]\n",
    "    dic['items'] = dic['text'][1:]\n",
    "\n",
    "# uncomment to test\n",
    "sections[0]['header']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Now that we have the header paragraph isolated, we can use that to derive the 'decision' of the section -- e.g. accept, reject, etc. \n",
    "\n",
    "For each dictinary in the `sections` list, add a called `decision` key with the decision of that section. Your results should look something like this (ignore the content details, and remember: dictionaries are not ordered):\n",
    "\n",
    "```\n",
    "```\n",
    "[\n",
    "    { \"paragraph-number\": 101,\n",
    "      \"full-text\": [ 101. Some text, 101.1 More text, 101.2 Another line, 101.3 Even more]\n",
    "      \"header\": 101. Some text\n",
    "      \"items\": [101.1 More text, 101.2 Another line, 101.3 Even more] \n",
    "      \"decision\": \"accept\" } ,\n",
    "     \n",
    "    { \"paragraph-number\": 102,\n",
    "      \"full-text\": [ 102. A new section, 102.1 Some text, 102.2 Even more text, 102.3 You get the picture ] \n",
    "      \"header\": 102. A new section\n",
    "      \"items\": [102.1 Some text, 102.2 Even more text, 102.3 You get the picture] \n",
    "      \"decision: \"reject\" }\n",
    "]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'support'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign a decision\n",
    "for dic in sections: \n",
    "    text = dic['header'] \n",
    "    decision = ''\n",
    "    if 'implemented' in text or 'process of implementation' in text:\n",
    "        decision = 'implemented'\n",
    "    elif 'will be examined' in text or 'will examine' in text or \"further examined\" in text or \"Responses to the following recommendations will be provided\" in text or \"will be included in the outcome report\" in text or \"will be provided in due course\" in text or \"course of the discussion\" in text:\n",
    "        decision = 'consider'\n",
    "    elif 'not enjoy the support' in text or 'reject' in text or 'cannot be accepted' in text:\n",
    "        decision = 'reject'\n",
    "    elif 'support' in text and 'did not enjoy the support' not in text:\n",
    "        decision = 'support'\n",
    "    elif 'have been noted by' in text or 'were noted by' in text:\n",
    "        decision = 'noted'\n",
    "    elif 'do not reflect the current situation' in text:\n",
    "        decision = 'reject'\n",
    "    else:\n",
    "        decision = 'unknown'\n",
    "    dic['decision'] = decision\n",
    "\n",
    "# uncomment to test\n",
    "sections[0]['decision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Making a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 We want to do the above parsing for each document in the list. Using the code you wrote in sections 1 and 2, write a function that passes a document, and returns an object `parsed_sections`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn it into a function\n",
    "def parse(document):\n",
    "    \n",
    "    upr = document\n",
    "    \n",
    "    # separate upr out into chunks by mainP, i.e. the support, reject, consider chunks \n",
    "    # by identifying subparagraphs that start with the main paragraph numbers.\n",
    "    sections = []\n",
    "    for n in mainParagraphs(upr):\n",
    "        dic = {}\n",
    "        dic['paragraph'] = n\n",
    "        dic['text'] = [s for s in upr['text'] if s.startswith(str(n))]\n",
    "        sections.append(dic)\n",
    "    \n",
    "    # delete all the chunks with only 1 paragraph\n",
    "    for dic in sections:\n",
    "        if len(dic['text']) == 1:\n",
    "            sections.remove(dic)\n",
    "    \n",
    "    # parse into main-text and items\n",
    "    for dic in sections:\n",
    "        dic['header'] = dic['text'][0]\n",
    "        dic['items'] = dic['text'][1:]\n",
    "    \n",
    "    # assign a decision\n",
    "    for dic in sections: \n",
    "        text = dic['header'] \n",
    "        decision = ''\n",
    "        if 'implemented' in text or 'process of implementation' in text:\n",
    "            decision = 'implemented'\n",
    "        elif 'will be examined' in text or 'will examine' in text or \"further examined\" in text or \"Responses to the following recommendations will be provided\" in text or \"will be included in the outcome report\" in text or \"will be provided in due course\" in text or \"course of the discussion\" in text:\n",
    "            decision = 'consider'\n",
    "        elif 'not enjoy the support' in text or 'reject' in text or 'cannot be accepted' in text:\n",
    "            decision = 'reject'\n",
    "        elif 'support' in text and 'did not enjoy the support' not in text:\n",
    "            decision = 'support'\n",
    "        elif 'have been noted by' in text or 'were noted by' in text:\n",
    "            decision = 'noted'\n",
    "        elif 'do not reflect the current situation' in text:\n",
    "            decision = 'reject'\n",
    "        else:\n",
    "            decision = 'unknown'\n",
    "        dic['decision'] = decision\n",
    "\n",
    "    return(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Loop through all the documents in the list `l` and assign a new `sections` key to the output from the above function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'136. The recommendations formulated during the interactive dialogue and listed below have been examined by Afghanistan and enjoy its support: '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply to all docs\n",
    "for i in l:\n",
    "    i['sections'] = parse(i)\n",
    "\n",
    "#uncomment to test\n",
    "l[0]['sections'][0]['header']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Getting the final list of recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Take a look at a recommendation. Notice that they're all formatted the same way, with the recommending country in parenthesis at the end, in between parentheses.\n",
    "\n",
    "Below, I've given you a sample recommendation. Find a way to pull out the recommending country. (Hint: Use the `split` method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136.1 To further build up on its effort to fully protect human rights in the country (Ethiopia); \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ethiopia'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec = l[0]['sections'][0]['items'][0] # the first line, from the first section, of the first upr in `l`\n",
    "print rec\n",
    "\n",
    "# your code here\n",
    "rec.split('(')[-1].split(')')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 We now want to create a new list `reclist` containing just the recommendations. Each recommendation should be a dictionary with the following keys: \n",
    "\n",
    "1. `to`: the country under review\n",
    "2. `from`: the country (or countries) giving the recommendation\n",
    "4. `year`: the year of the review (all 2014 here)\n",
    "5. `decision`: whether the recommendation was accepted, rejected, etc.\n",
    "6. `text`: the text of the recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decision': 'support',\n",
       " 'from': 'Indonesia',\n",
       " 'text': '136.2 Continue and deepen efforts to firmly root human rights values and principles in the Government system, including through human rights training to state officials (Indonesia); ',\n",
       " 'to': 'afghanistan',\n",
       " 'year': '2014'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dictionaries for each individual recommendation item\n",
    "reclist = []\n",
    "for upr in l:\n",
    "    for section in upr['sections']:\n",
    "        for item in section['items']:\n",
    "            dic = {}\n",
    "            dic['to'] = upr['country']\n",
    "            dic['year'] = upr['year']\n",
    "            dic['decision'] = section['decision']\n",
    "            dic['from'] = item.split('(')[-1].split(')')[0]\n",
    "            dic['text'] = item\n",
    "            reclist.append(dic)  \n",
    "\n",
    "# uncomment to test\n",
    "print len(reclist)\n",
    "reclist[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#writing column headings\n",
    "keys = l[0].keys()\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#writing the rest\n",
    "with open('2013data.csv', 'wb') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(recslist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
