---
title: 'Problem Set: Text Analysis'
author: "Rochelle Terman"
date: "June 17, 2015"
output: html_document
---
```{r}
setwd("~/Dropbox/berkeley/PS239T/text-analysis")
rm(list=ls())

library(tm)
library(RTextTools)
library("matrixStats")
```

# 1. PreProcessing

i. Load the data "women.csv" as a dataframe.

```{r}
documents <-read.csv("Data/women.csv", header=TRUE) #read in CSV file
names(documents)
```

ii. Make a Document-Term Matrix from the `text` column, making sure to remove punctuation and numbers, making everything lowercase, stemming the words, and removing sparse words at the .9 threshhold.

```{r}
dtm <- create_matrix(documents[["text"]], language="english", removeNumbers=TRUE,
                       stemWords=TRUE, removeSparseTerms=.9, toLower = TRUE, 
                       removePunctuation = TRUE)
```

iii. Which words apper most frequently? Which word is most highly correlated with "work"?

```{r}
# have a look at common words
findFreqTerms(dtm, lowfreq = 4000)
findAssocs(dtm, "work", 0.3)
```

iv. Coerce the DTM into a data.frame and add a column for `region` metadata.

```{r}
# coerce into data.farme
dtm <- as.data.frame(as.matrix(dtm))
# add meta column for region
dtm$document_region <- documents$region
# check to see if they're the same number of documents per author
summary(dtm$document_region)
summary(documents$region)
```

#2: Discriminating Words

In class, you saw an exercise finding distinctive words between two sub-corpora. The same method can be applied for >2 sub-corpora, by comparing one sub-corpura to the entire corpus. 

i. Find the most 10 most distinctive words for the `MENA` region.

```{r}
# subset
mena <- subset(dtm,document_region=="MENA",select = -document_region)
all <- subset(dtm,document_region!="MENA",select=-document_region)

n.mena <- sum(colSums(mena))
n.all <- sum(colSums(all))
means.mena <- colMeans(mena)
var.mena <- colVars(as.matrix(mena))
means.all <- colMeans(all)
var.all <- colVars(as.matrix(all))
  
#calculate overall score
score <- (means.mena - means.all) / sqrt((var.mena/n.mena) + (var.all/n.mena))

# sort and view
score <- sort(score)
mena_top <- tail(score,10)
mena_top
```

ii. Generalize the code above to a function that passes a region as a string, like "MENA" or "Africa" and returns the most distinctive words for that region. Use the function to find the distinctive words for the "LA" (Latin America) region.

```{r}
distinctive.words <- function(region){
  subC = dtm[dtm$document_region==region,]
  subC$document_region <- NULL
  all <- dtm[dtm$document_region!=region,]
  all$document_region <- NULL

  n.subC <- sum(colSums(subC))
  n.all <- sum(colSums(all))
  means.subC <- colMeans(subC)
  var.subC <- colVars(as.matrix(subC))
  means.all <- colMeans(all)
  var.all <- colVars(as.matrix(all))
  
  #calculate overall score
  score <- (means.subC - means.all) / sqrt((var.subC/n.subC) + (var.all/n.subC))

  # sort and view
  score <- sort(score)
  subC_top <- tail(score,10)
  return(subC_top)
}

distinctive.words("LA")
```

#3: Structural Topic Model

i. Install the `stm` package and read the help file carefully. Estimate a topic model with 15 topics that estimates on the `region` of origin for topic prevalence. In your model, set `lower.thresh` to 50 to increase efficiency. (This will keep only words that are in at least 50 documents.) **Warning**: This will take awhile.

```{r}
library(stm)

# process
temp<-textProcessor(documents=documents$text,metadata=documents)
meta<-temp$meta
vocab<-temp$vocab
docs<-temp$documents
out <- prepDocuments(docs, vocab, meta, lower.thresh=50)
docs<-out$documents
vocab<-out$vocab
meta <-out$meta

# estimate STM
model <- stm(docs,vocab, 15, prevalence=~region, data=meta, seed = 00001)
```

ii. Use `labeltopics` to label each of the topics. 

```{r}
labelTopics(model)
```

iii. Using `estimateEffect`, estimate the effect of "region" on topics. Using `plot.estimateEffect`, create a point estimate plot of the effect of region on topic prevalence for one topic. 

```{r}
# estimate effect
prep <- estimateEffect(1:15 ~ region,model,meta=meta,uncertainty="Global")

# plot effect
regions = c("Asia","EECA","MENA","Africa","LA","West")

plot.estimateEffect(prep,"region",method="pointestimate",topics=10,printlegend=TRUE,labeltype="custom",custom.labels=regions,main="Regional effects on Topic 10",ci.level=.9)
```

## 4. QDAP

Using the `qdap` package, do a sentiment analysis (and visualization) of a corpus of your choosing. You can compare sentiments across time, speaker, song, artist, whatever. Be creative. If you're feeling brave, you can use another tool in the `qdap` package besides `polarity` to show something interesting about a corpus. 


