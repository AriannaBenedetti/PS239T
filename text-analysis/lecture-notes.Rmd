Text Analysis Lecture Notes + Code
========================================================

### Credits: 
1. A lot of the preprocessing stuff came from [here](http://onepager.togaware.com/TextMiningO.pdf)
2. The clustering bit at the end came from Ben Marwick's [Day of Archaeology work](https://github.com/benmarwick/dayofarchaeology)

First load our required packages:
```{r}
setwd("~/Dropbox/berkeley/PS239T/text-analysis")
rm(list=ls())
library(tm) # Framework for text mining
library(RTextTools) # a machine learning package for text classification written in R
library(mallet) # A wrapper around the Java machine learning tool MALLET
library(qdap) # Quantiative discourse analysis of transcripts
library(qdapDictionaries)
library(dplyr) # Data preparation and pipes $>$
library(ggplot2) # for plotting word frequencies
library(SnowballC) # for stemming
```

## 1. Prepare a Corpus

A corpus is a collection of texts, usually stored electronically, and from which we perform our analysis. A corpus might be a collection of news articles from Reuters or the published works of Shakespeare. Within each corpus we will have separate articles, stories, volumes, each treated as a separate entity or record.

Documents come in a variety of formats, but plain text is best.

### 1.1 Corpus Sources and Readers

The `tm` package supports a variety of sources and formats. 

```{r}
getSources()
getReaders()
```

We can read a corpus from a directory that contains text files, each document a different file
```{r}
documents <- Corpus(DirSource("Data/Sessions")) 
documents
```

Or we can read from a csv of documents, with each row being a document, and columns for metadata (information about each document)

```{r}
documents <-read.csv("Data/shelbysessions.csv", header=TRUE) #read in CSV file
docs <- Corpus(VectorSource(documents$text))
docs
```

We can inspect the documents using inspect()

```{r}
inspect(docs[16])
```
### 1.2 Preprocessing

Many text analysis applications follow a similar 'recipe' for preprecessing, involving:

1. Tokenizing the text to unigrams (or bigrams, or trigrams)
2. Converting all characters to lowercase
3. Removing punctuation
4. Removing numbers
5. Removing Stop Words, inclugind custom stop words
6. "Stemming" words, or lemmitization. There are several stemming alogrithms. Porter is the most popular.
7. Removing Sparse Terms
8. Creating a Document-Term Matrix

See what transformations are available TM package

```{r}
getTransformations()
```

The function `tm.map()` is used to apply one of these transformations across all documents. 

We can also use regular R functions for custom transformations using the `content_transformer()` function.

For example, we might want to replace “/”, used sometimes to separate alternative words, with a space. This will avoid the two words being run into one string of characters through the transformations. We might also replace “@” and “|” with a space, for the same reason
```{r}
# note: won't work on past versions of R
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace,"\\|")
docs[[100]]
```
We can basic transformations using the build in `tm_map` tools.

```{r}
docs <- tm_map(docs, content_transformer(tolower)) # convert all text to lower case
docs <- tm_map(docs, removePunctuation) # remove Puncturation
docs <- tm_map(docs, removeNumbers) # remove Numbers
docs <- tm_map(docs, removeWords, stopwords("english")) # remove common words
stopwords("english") # check out what was removed
docs <- tm_map(docs, removeWords, c("shelby","sessions")) # remove own stop words
docs <- tm_map(docs, stripWhitespace) # strip white space
docs <- tm_map(docs, stemDocument) # stem the document
inspect(docs[16])
```

### 1.3 Creating a DTM

A document term matrix is simply a matrix with documents as the rows and terms as the columns and a count of the frequency of words as the cells of the matrix. We use `DocumentTermMatrix()` to create the matrix:
```{r}
dtm <- DocumentTermMatrix(docs)
dtm
dim(dtm) # how many documents? how many terms?
inspect(dtm[1:5,1000:1005]) # take a quick look
```

Note: we can use RTextTools to go directly from text vector to DTM, which is probably easier.

```{r}
# make DTM
dtm <- create_matrix(docs[["text"]], language="english", removeNumbers=TRUE,
                       stemWords=TRUE, removeSparseTerms=.9, toLower = TRUE, 
                       removePunctuation = TRUE)
```

### 1.4 Exploring the DTM

We can obtain the term frequencies as a vector by converting the document term matrix into a matrix and summing the column counts:

```{r}
freq <- colSums(as.matrix(dtm))
length(freq) # how many terms?
```
By ordering the frequencies we can list the most frequent terms and the least frequent terms:
```{r}
# order
ord <- order(freq)

# Least frequent terms
freq[head(ord)]

# most frequent
freq[tail(ord)]

# frequency of frenquencies
head(table(freq),15)
tail(table(freq),15)

# plot
plot(table(freq))
```

We can remove sparse terms and thus inrease efficency

```{r}
dtm.s <- removeSparseTerms(dtm,.99)
dtm # 6506 terms
dtm.s# 1849 terms
```

Exploring word frequences

```{r}
# Have a look at common words
findFreqTerms(dtm, lowfreq=1000) # words that appear at least 1000 times

# Which words correlate with "congress"?
findAssocs(dtm, "fire", 0.7)

# plot
freq <- sort(colSums(as.matrix(dtm)),decreasing=TRUE)
head(freq)

wf <- data.frame(word=names(freq), freq=freq)
head(wf)

library(ggplot2)
subset(wf, freq>1000) %>%
  ggplot (aes(word, freq)) +
  geom_bar (stat ="identity") +
  theme(axis.text.x=element_text(angle=45,hjust=1))

# wordcoulds!
library(wordcloud)
set.seed(123)
wordcloud(names(freq), freq, max.words=100, colors=brewer.pal(6,"Dark2"))
```

We can convert a DTM to a matrix or data.frame in order to write to a csv, add meta data, etc.

```{r}
# coerce into dataframe
dtm <- as.data.frame(as.matrix(dtm))
names(docs)

# add column for author
dtm$document_author <- documents$author

# check to see if they're the same number of documents per author
summary(dtm$document_author)
summary(documents$author)
```
## 2. Sentiment Analysis 

```{r}
# load documents # TODO: FIND BETTER DATA.
documents <-read.csv("Data/ceos_newyorktimes_data.csv", header=TRUE) #read in CSV

# Split the text into sentences
# sentences<-sentSplit(dataframe=text.df, text.var="text")

# Compute various polarity (sentiment) statistics 
polarity.score<-polarity(text.var=documents$TEXT)
polarity.score$group[,c("total.sentences", "total.words", "ave.polarity")]
polarity.score$all[1:20, c("pos.words", "neg.words")]
#TODO: Visualize this somehow.
```

## 3. Discriminating Word Analysis

This is one method to calculate discriminating words. There are many ways to do this. Here we use a standardized mean difference score. It's like a difference of proportion analysis but normalized, as per the following equation:

First we have to split the data.frame to two DTMs from the two authors.

```{r}
# Subset into 2 dtms
shelby <- subset(dtm,document_author=="Shelby",select = -document_author) # should have 1102 obs.
sessions <- subset(dtm,document_author=="Sessions",select = -document_author) # should have 236 obs.
```

Then we calculate the scores. 

```{r}
# calculate means and vars
n.shelby <- sum(colSums(shelby))
n.sessions <- sum(colSums(sessions))
means.shelby <- colMeans(shelby)
var.shelby <- colVars(as.matrix(shelby))
means.sessions <- colMeans(sessions)
var.sessions <- colVars(as.matrix(sessions))
  
#calculate overall score
score <- (means.shelby - means.sessions) / sqrt((var.shelby/n.shelby) + (var.sessions/n.shelby))

# sort and view
score <- sort(score)
head(score,10) # top session words
tail(score,10) # top shelby words
```

## 4. Mallet

Is a popular topic modelling software application. Mallet is in java - which means it's fast - but R has a Mallet wrapper package.

```{r}
# we first have to create an 'id' column
documents$id <- rownames(documents)

# load data
mallet.instances <- mallet.import(documents$id, documents$text, "Data/stoplist.csv", FALSE, token.regexp="[\\p{L}']+")

## Create a topic trainer object.
topic.model <- MalletLDA(num.topics=15)

## Load our documents
topic.model$loadDocuments(mallet.instances)

## Get the vocabulary, and some statistics about word frequencies.
##  These may be useful in further curating the stopword list.
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)

# examine some of the vocabulary
word.freqs[1:50,]

## Optimize hyperparameters every 20 iterations, 
##  after 50 burn-in iterations.
topic.model$setAlphaOptimization(20, 50)

## Now train a model. Note that hyperparameter optimization is on, by default.
##  We can specify the number of iterations. Here we'll use a large-ish round number.
topic.model$train(200)

## Get the probability of topics in documents and the probability of words in topics.
## By default, these functions return raw word counts. Here we want probabilities, 
##  so we normalize, and add "smoothing" so that nothing has exactly 0 probability.
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)

## What are the top words in topic 7?
##  Notice that R indexes from 1, so this will be the topic that mallet called topic 6.
mallet.top.words(topic.model, topic.words[6,])

## Get a vector containing short names for the topics
n.topics <- 15
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) 
  topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")

# have a look at keywords for each topic
topics.labels

## Show the first few document titles with at least .25 of its content devoted to topic 6
head(documents[ doc.topics[6,] > 0.25 , ],)

## Show title of the most representative text for topic 6
documents[which.max(doc.topics[6,]),]$title

## How do topics differ across different sub-corpora?
mena.topic.words <- mallet.subset.topic.words(topic.model, documents$region == "MENA", smoothed=T, normalized=T)
west.topic.words <- mallet.subset.topic.words(topic.model, documents$region == "West", smoothed=T, normalized=T)

mallet.top.words(topic.model, mena.topic.words[8,])
mallet.top.words(topic.model, west.topic.words[8,])
```

We can visualize topics as words clouds.

```{r}
# be sure you have installed the wordcloud package
library(wordcloud)
topic.num <- 1
num.top.words<-100
topic.top.words <- mallet.top.words(topic.model, topic.words[1,], 100)
wordcloud(topic.top.words$words, topic.top.words$weights, c(4,.8), rot.per=0, random.order=F)

num.topics<-15
num.top.words<-25
for(i in 1:num.topics){
  topic.top.words <- mallet.top.words(topic.model, topic.words[i,], num.top.words)
  wordcloud(topic.top.words$words, topic.top.words$weights, c(4,.8), rot.per=0, random.order=F)
}
```

## Clustering

```{r}
#####################
# Clustering
# from https://github.com/shawngraham/R/blob/master/topicmodel.R
#####################

# from http://www.cs.princeton.edu/~mimno/R/clustertrees.R
## transpose and normalize the doc topics
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)

# create data.frame with columns as people and rows as topics
topic_docs <- data.frame(topic.docs)
names(topic_docs) <- documents$id

## cluster based on shared words
plot(hclust(dist(topic.words)), labels=topics.labels)

#' Calculate similarity matrix
#' Shows which documents are similar to each other
#' by their proportions of topics. Based on Matt Jockers' method
library(cluster)
topic_df_dist <- as.matrix(daisy(t(topic_docs), metric = "euclidean", stand = TRUE))

# Change row values to zero if less than row minimum plus row standard deviation
# keep only closely related documents and avoid a dense spagetti diagram
# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0

#' Use kmeans to identify groups of similar authors
km <- kmeans(topic_df_dist, n.topics)

# get names for each cluster
allnames <- vector("list", length = n.topics)
for(i in 1:n.topics){
  allnames[[i]] <- names(km$cluster[km$cluster == i])
} 

# Visualize people similarity using force-directed network graphs

#### network diagram using Fruchterman & Reingold algorithm
# static
# if you don't have igraph, install it by removing the hash below:
# install.packages("igraph")
library(igraph)
g <- as.undirected(graph.adjacency(topic_df_dist))
layout1 <- layout.fruchterman.reingold(g, niter=500)
plot(g, layout=layout1, edge.curved = TRUE, vertex.size = 1, vertex.color= "grey", edge.arrow.size = 0, vertex.label.dist=0.5, vertex.label = NA)


# interactive in a web browser
# if you have a particularly large dataset, you might want to skip this section, and just run the Gephi part.
# if you don't have devtools, install it by removing the hash below:
# install.packages("devtools")

devtools::install_github("christophergandrud/networkD3")
require(d3Network)
d3SimpleNetwork(get.data.frame(g),width = 1500, height = 800,
                textColour = "orange", linkColour = "red",
                fontsize = 10,
                nodeClickColour = "#E34A33",
                charge = -100, opacity = 0.9, file = "d3net.html")

# find the html file in working directory and open in a web browser

```
