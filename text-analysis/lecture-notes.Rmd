Text Analysis Lecture Notes + Code
========================================================

This is an R Markdown document. Markdown is a simple formatting syntax for authoring web pages (click the **Help** toolbar button for more details on using R Markdown).

When you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

## Sentiment Analysis 

```{r}
# load documents
setwd("~/Dropbox/berkeley/PS239T/text-analysis")
documents <-read.csv("Data/ceos_newyorktimes_data.csv", header=TRUE) #read in CSV

# Install the "qdap" package (only necessary one time)
# install.packages("qdap") # Not Run

# Load the "qdap" package (necessary every new R session)
library(qdap)

# Split the text into sentences
# sentences<-sentSplit(dataframe=text.df, text.var="text")

# Compute various polarity (sentiment) statistics 
polarity.score<-polarity(text.var=documents$TEXT)
polarity.score$group[,c("total.sentences", "total.words", "ave.polarity")]
polarity.score$all[1:20, c("pos.words", "neg.words")]
```
## Preprocessing

The first step in text analysis is creating a corpus (or corpora) of texts. This can be as simple as a csv file, with one column of `text` and other columsn for metadata.

Many text analysis applications follow a similar 'recipe' for preprecessing, involving:

1. Tokenizing the text to unigrams (or bigrams, or trigrams)
2. Converting all characters to lowercase
3. Removing punctuation
4. Removing numbers
5. Removing Stop Words, inclugind custom stop words
6. "Stemming" words, or lemmitization. There are several stemming alogrithms. Porter is the most popular.
7. Removing Sparse Terms
8. Creating a Document-Term Matrix

In R, there are many ways to do this.

```{r}
# Preprocessing using TM package

# Reading from a directory that contains just text files, each document is a different file.
# a <- Corpus(DirSource("Data/Sessions")) 

# reading from a csv
documents <-read.csv("Data/ceos_newyorktimes_data.csv", header=TRUE) #read in CSV file
a <- Corpus(VectorSource(documents$text))

# preprocessing
a <- tm_map(a, content_transformer(tolower)) # convert all text to lower case
a <- tm_map(a, removePunctuation) 
a <- tm_map(a, removeNumbers)
a <- tm_map(a, removeWords, stopwords("english"))
a <- tm_map(a, stemDocument)

# creating a DTM
dtm.sessions <- DocumentTermMatrix(a)
```

Alternatively, we can use RTextTools, which is probably easier.

```{r}
# make DTM
dtm <- create_matrix(documents[["text"]], language="english", removeNumbers=TRUE,
                       stemWords=TRUE, removeSparseTerms=.9, toLower = TRUE, 
                       removePunctuation = TRUE)

# Have a look at common words
findFreqTerms(dtm, lowfreq=100)

# Which words correlate with "congress"?
findAssocs(dtm, "congress", 0.2)
```

We can easily add metadata in the form of columns by turning it into a data.frame.

```{r}
# coerce into dataframe
dtm <- as.data.frame(inspect(dtm)) # 275 terms

# add column for author
dtm$document_author <- documents$author

# check to see if they're the same number of documents per author
summary(dtm$document_author)
summary(documents$author)
```

## Discriminating Word Analysis

This is one method to calculate discriminating words. There are many ways to do this. Here we use a standardized mean difference score. It's like a difference of proportion analysis but normalized, as per the following equation:

First we have to split the data.frame to two DTMs from the two authors.

```{r}
# Subset into 2 dtms
shelby <- subset(dtm,document_author=="Shelby",select = -document_author) # should have 1102 obs.
sessions <- subset(dtm,document_author=="Sessions",select = -document_author) # should have 236 obs.
```

Then we calculate the scores. 

```{r}
# calculate means and vars
n.shelby <- sum(colSums(shelby))
n.sessions <- sum(colSums(sessions))
means.shelby <- colMeans(shelby)
var.shelby <- colVars(as.matrix(shelby))
means.sessions <- colMeans(sessions)
var.sessions <- colVars(as.matrix(sessions))
  
#calculate overall score
score <- (means.shelby - means.sessions) / sqrt((var.shelby/n.shelby) + (var.sessions/n.shelby))

# sort and view
score <- sort(score)
head(score,10) # top session words
tail(score,10) # top shelby words
```

## Mallet

Is a popular topic modelling software application. Mallet is in java - which means it's fast - but R has a Mallet wrapper package.

```{r}
library(mallet)
# we first have to create an 'id' column
documents$id <- rownames(documents)

# load data
mallet.instances <- mallet.import(documents$id, documents$text, "Data/stoplist.csv", FALSE, token.regexp="[\\p{L}']+")

## Create a topic trainer object.
topic.model <- MalletLDA(num.topics=15)

## Load our documents
topic.model$loadDocuments(mallet.instances)

## Get the vocabulary, and some statistics about word frequencies.
##  These may be useful in further curating the stopword list.
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)

# examine some of the vocabulary
word.freqs[1:50,]

## Optimize hyperparameters every 20 iterations, 
##  after 50 burn-in iterations.
topic.model$setAlphaOptimization(20, 50)

## Now train a model. Note that hyperparameter optimization is on, by default.
##  We can specify the number of iterations. Here we'll use a large-ish round number.
topic.model$train(200)

## Get the probability of topics in documents and the probability of words in topics.
## By default, these functions return raw word counts. Here we want probabilities, 
##  so we normalize, and add "smoothing" so that nothing has exactly 0 probability.
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)

## What are the top words in topic 7?
##  Notice that R indexes from 1, so this will be the topic that mallet called topic 6.
mallet.top.words(topic.model, topic.words[6,])

## Get a vector containing short names for the topics
n.topics <- 15
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) 
  topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")

# have a look at keywords for each topic
topics.labels

## Show the first few document titles with at least .25 of its content devoted to topic 6
head(documents[ doc.topics[6,] > 0.25 , ],)

## Show title of the most representative text for topic 6
documents[which.max(doc.topics[6,]),]$title

## How do topics differ across different sub-corpora?
mena.topic.words <- mallet.subset.topic.words(topic.model, documents$region == "MENA", smoothed=T, normalized=T)
west.topic.words <- mallet.subset.topic.words(topic.model, documents$region == "West", smoothed=T, normalized=T)

mallet.top.words(topic.model, mena.topic.words[8,])
mallet.top.words(topic.model, west.topic.words[8,])
```

Topics can be visualized in a number of ways

```
# Visualize topics as word clouds
# be sure you have installed the wordcloud package
library(wordcloud)
topic.num <- 1
num.top.words<-100
topic.top.words <- mallet.top.words(topic.model, topic.words[1,], 100)
wordcloud(topic.top.words$words, topic.top.words$weights, c(4,.8), rot.per=0, random.order=F)

num.topics<-15
num.top.words<-25
for(i in 1:num.topics){
  topic.top.words <- mallet.top.words(topic.model, topic.words[i,], num.top.words)
  wordcloud(topic.top.words$words, topic.top.words$weights, c(4,.8), rot.per=0, random.order=F)
}
```

We can cluster documents together based on their topic proportions.

```{r}
#####################
# Clustering
# from https://github.com/shawngraham/R/blob/master/topicmodel.R
#####################

# from http://www.cs.princeton.edu/~mimno/R/clustertrees.R
## transpose and normalize the doc topics
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)

# create data.frame with columns as people and rows as topics
topic_docs <- data.frame(topic.docs)
names(topic_docs) <- documents$id

## cluster based on shared words
plot(hclust(dist(topic.words)), labels=topics.labels)

#' Calculate similarity matrix
#' Shows which documents are similar to each other
#' by their proportions of topics. Based on Matt Jockers' method
library(cluster)
topic_df_dist <- as.matrix(daisy(t(topic_docs), metric = "euclidean", stand = TRUE))

# Change row values to zero if less than row minimum plus row standard deviation
# keep only closely related documents and avoid a dense spagetti diagram
# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0

#' Use kmeans to identify groups of similar authors
km <- kmeans(topic_df_dist, n.topics)

# get names for each cluster
allnames <- vector("list", length = n.topics)
for(i in 1:n.topics){
  allnames[[i]] <- names(km$cluster[km$cluster == i])
} 

# Visualize people similarity using force-directed network graphs

#### network diagram using Fruchterman & Reingold algorithm
# static
# if you don't have igraph, install it by removing the hash below:
# install.packages("igraph")
library(igraph)
g <- as.undirected(graph.adjacency(topic_df_dist))
layout1 <- layout.fruchterman.reingold(g, niter=500)
plot(g, layout=layout1, edge.curved = TRUE, vertex.size = 1, vertex.color= "grey", edge.arrow.size = 0, vertex.label.dist=0.5, vertex.label = NA)


# interactive in a web browser
# if you have a particularly large dataset, you might want to skip this section, and just run the Gephi part.
# if you don't have devtools, install it by removing the hash below:
# install.packages("devtools")

devtools::install_github("christophergandrud/networkD3")
require(d3Network)
d3SimpleNetwork(get.data.frame(g),width = 1500, height = 800,
                textColour = "orange", linkColour = "red",
                fontsize = 10,
                nodeClickColour = "#E34A33",
                charge = -100, opacity = 0.9, file = "d3net.html")

# find the html file in working directory and open in a web browser

```
